{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow_addons.text.crf import crf_log_likelihood, viterbi_decode\n",
    "# Load and preprocess data\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "def preprocess_data(text, entities):\n",
    "    tokens = text.split()  # Simple whitespace tokenization\n",
    "    labels = [entities.get(token, 'O') for token in tokens]\n",
    "    return tokens, labels\n",
    "# Define CRF Layer\n",
    "class CRFLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_tags, **kwargs):\n",
    "        super(CRFLayer, self).__init__(**kwargs)\n",
    "        self.num_tags = num_tags\n",
    "    def build(self, input_shape):\n",
    "        self.transitions = self.add_weight(\n",
    "            name=\"transitions\",\n",
    "            shape=(self.num_tags, self.num_tags),\n",
    "            initializer=\"glorot_uniform\"\n",
    "        )\n",
    "        super(CRFLayer, self).build(input_shape)\n",
    "    def call(self, logits):\n",
    "        return logits\n",
    "    def get_loss(self, logits, labels, sequence_lengths):\n",
    "        sequence_lengths = tf.cast(sequence_lengths, tf.int32)\n",
    "        log_likelihood, _ = crf_log_likelihood(\n",
    "            logits,\n",
    "            labels,\n",
    "            sequence_lengths,\n",
    "            self.transitions\n",
    "        )\n",
    "        return -tf.reduce_mean(log_likelihood)\n",
    "# Decode CRF predictions\n",
    "def crf_decode(logits, sequence_lengths, transitions):\n",
    "    \"\"\"Decodes logits using Viterbi algorithm.\"\"\"\n",
    "    decoded_sequences = []\n",
    "    for logit, seq_len in zip(logits, sequence_lengths):\n",
    "        viterbi_path, _ = viterbi_decode(logit[:seq_len], transitions)\n",
    "        decoded_sequences.append(viterbi_path)\n",
    "    return decoded_sequences\n",
    "# Load and process data\n",
    "def load_and_process_data(file_path, entities_example, max_len=50):\n",
    "    text = load_text(file_path)\n",
    "    tokens, labels = preprocess_data(text, entities_example)\n",
    "    # Encode tokens and labels\n",
    "    word_encoder = LabelEncoder()\n",
    "    word_encoder.fit(tokens)\n",
    "    encoded_tokens = word_encoder.transform(tokens)\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels)\n",
    "    encoded_labels = label_encoder.transform(labels)\n",
    "    # Pad sequences for model input\n",
    "    X = tf.keras.preprocessing.sequence.pad_sequences([encoded_tokens], maxlen=max_len, padding='post')\n",
    "    Y = tf.keras.preprocessing.sequence.pad_sequences([encoded_labels], maxlen=max_len, padding='post')\n",
    "    return X, Y, tokens, label_encoder\n",
    "# Define entities dictionary for NER classification\n",
    "entities_example = {\n",
    "    'discovery': 'B-EVENT',\n",
    "    'Italian': 'B-LOCATION',\n",
    "    'Wired': 'B-ORG',\n",
    "    'Aimee': 'B-PERSON',\n",
    "    'Mullins': 'I-PERSON',\n",
    "}\n",
    "label_list = [\n",
    "    'O', 'B-PERSON', 'I-PERSON', 'B-ORG', 'I-ORG', 'B-LOCATION', 'I-LOCATION', 'B-EVENT', 'I-EVENT'\n",
    "]\n",
    "# Load and process data\n",
    "max_len = 1028\n",
    "X_train, Y_train, train_tokens, label_encoder = load_and_process_data(\"transcription_train.txt\", entities_example, max_len)\n",
    "X_test, Y_test, test_tokens, _ = load_and_process_data(\"transcription_test_AimeeMullins_1249s.txt\", entities_example, max_len)\n",
    "# Define model\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(input_dim=10000, output_dim=1028, mask_zero=True)(input_layer)\n",
    "bi_lstm_layer = Bidirectional(LSTM(1028, return_sequences=True))(embedding_layer)\n",
    "dense_layer = TimeDistributed(Dense(len(label_list)))(bi_lstm_layer)\n",
    "# CRF Layer\n",
    "crf_layer = CRFLayer(num_tags=len(label_list))\n",
    "logits = crf_layer(dense_layer)\n",
    "# Build model\n",
    "model = Model(inputs=input_layer, outputs=logits)\n",
    "# Compile with dummy loss (manual loss handling below)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\")\n",
    "# Compute sequence lengths\n",
    "sequence_lengths = tf.reduce_sum(tf.cast(tf.not_equal(X_train, 0), tf.int32), axis=-1)\n",
    "# Training step with custom CRF loss\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss = crf_layer.get_loss(logits, y, sequence_lengths)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    loss = train_step(X_train, Y_train)\n",
    "    print(f\"Loss: {loss.numpy()}\")\n",
    "# Decoding predictions\n",
    "sequence_lengths_test = tf.reduce_sum(tf.cast(tf.not_equal(X_test, 0), tf.int32), axis=-1).numpy()\n",
    "predictions = model.predict(X_test)\n",
    "decoded_predictions = crf_decode(predictions, sequence_lengths_test, crf_layer.transitions)\n",
    "# Map predictions back to labels\n",
    "decoded_labels = []\n",
    "for pred in decoded_predictions:\n",
    "    decoded_labels.append(label_encoder.inverse_transform(pred))\n",
    "# Display tokens and predicted NER labels\n",
    "for token, label in zip(test_tokens, decoded_labels[0]):\n",
    "    print(f\"Token: {token}, Predicted Label: {label}\")\n",
    " \n",
    "has context menu"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
