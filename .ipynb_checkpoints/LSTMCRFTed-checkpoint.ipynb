{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb18828",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fb18828",
    "outputId": "a42b4ac9-670d-43f1-aedd-78e58dd94cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\heetk\\miniconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: TorchCRF in c:\\users\\heetk\\miniconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (3.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from TorchCRF) (2.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\heetk\\miniconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\heetk\\miniconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (3.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in c:\\users\\heetk\\miniconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\heetk\\miniconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in c:\\users\\heetk\\miniconda3\\lib\\site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\heetk\\miniconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\heetk\\miniconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: datasets[audio] in c:\\users\\heetk\\miniconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets[audio]) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets[audio]) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets[audio]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets[audio]) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets[audio]) (3.10.10)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets[audio]) (0.12.1)\n",
      "Requirement already satisfied: librosa in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets[audio]) (0.10.2.post1)\n",
      "Requirement already satisfied: soxr>=0.4.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from datasets[audio]) (0.5.0.post1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets[audio]) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets[audio]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets[audio]) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets[audio]) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets[audio]) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from aiohttp->datasets[audio]) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from soundfile>=0.12.1->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from librosa->datasets[audio]) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from librosa->datasets[audio]) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from librosa->datasets[audio]) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from librosa->datasets[audio]) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from librosa->datasets[audio]) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from librosa->datasets[audio]) (1.8.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from librosa->datasets[audio]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from librosa->datasets[audio]) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from pandas->datasets[audio]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from pandas->datasets[audio]) (2024.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from pooch>=1.1->librosa->datasets[audio]) (3.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets[audio]) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: soundfile in c:\\users\\heetk\\miniconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\heetk\\miniconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch TorchCRF\n",
    "%pip install torch torchaudio\n",
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install --upgrade pip\n",
    "%pip install --upgrade transformers accelerate datasets[audio]\n",
    "%pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937d8d19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "937d8d19",
    "outputId": "bae5298e-bf04-4056-cb8b-aef8f4ca4974"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found: transcription_test_AimeeMullins_1249s.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(local_file_path):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File not found: transcription_test_AimeeMullins_1249s.txt"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from TorchCRF import CRF\n",
    "\n",
    "# Define the path to your local file\n",
    "local_file_path = \"transcription_test_AimeeMullins_1249s_summarized.txt\"\n",
    "\n",
    "# Verify the file exists\n",
    "import os\n",
    "if not os.path.exists(local_file_path):\n",
    "    raise FileNotFoundError(f\"File not found: {local_file_path}\")\n",
    "else:\n",
    "    print(f\"File '{local_file_path}' found.\")\n",
    "\n",
    "# Load the dataset using the appropriate loader based on your file type\n",
    "tedlium = load_dataset(\"text\", data_files=local_file_path)\n",
    "\n",
    "# Verify the dataset is loaded correctly\n",
    "print(tedlium)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class TEDLIUMDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transcription = self.texts[idx]\n",
    "        return transcription\n",
    "\n",
    "# Prepare the dataset\n",
    "train_texts = tedlium['train']['text']\n",
    "\n",
    "# Creating the dataset\n",
    "train_dataset = TEDLIUMDataset(train_texts)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)  # Adjust batch size as needed\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "embed_dim = 100  # Dimension of the embedding layer\n",
    "hidden_dim = 128  # Dimension of LSTM hidden states\n",
    "output_dim = 2  # Number of classes for classification (adjust as necessary)\n",
    "\n",
    "# Define the LSTM-CRF model\n",
    "class LSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(LSTMCRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=tokenizer.pad_token_id)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply by 2 for bidirectional\n",
    "\n",
    "        # CRF layer for sequence tagging (without batch_first argument)\n",
    "        self.crf = CRF(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "    def decode(self, logits, mask):\n",
    "        return self.crf.decode(logits, mask=mask)\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMCRF(vocab_size, embed_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model is running on {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Tokenize the batch of texts\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        # Placeholder for labels (replace this with actual labels)\n",
    "        labels = torch.randint(0, output_dim, (input_ids.size(0), input_ids.size(1))).to(device)  # Random labels for testing\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)  # Forward pass\n",
    "\n",
    "        # Compute CRF loss\n",
    "        try:\n",
    "            loss = -model.crf(logits, labels, mask=attention_mask.bool())  # Negative log likelihood\n",
    "            print(f\"Loss shape: {loss.shape}\")  # Print shape of loss to check\n",
    "\n",
    "            if loss.ndimension() > 0:\n",
    "                loss = loss.mean()  # Reduce to scalar if not already scalar\n",
    "\n",
    "            print(f\"Reduced Loss: {loss}\")  # Print reduced loss value\n",
    "\n",
    "            total_loss += loss.item()  # Convert to scalar and accumulate\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # After training, make predictions using the CRF layer (using viterbi_decode)\n",
    "            predicted_labels, _ = model.crf.viterbi_decode(logits, mask=attention_mask.bool())  # Decode using CRF\n",
    "            print(\"Predicted Labels:\", predicted_labels)\n",
    "            break  # Remove to evaluate on the entire dataset\n",
    "        except IndexError as e:\n",
    "            print(f\"Skipping batch with shape mismatch: logits shape {logits.shape}, labels shape {labels.shape}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)  # Average loss for the epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Sample prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        predicted_labels, _ = model.crf.viterbi_decode(logits, mask=attention_mask.bool())  # Decode using CRF\n",
    "        print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "        break  # Remove to evaluate on the entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "WyjCGjDw8Su4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyjCGjDw8Su4",
    "outputId": "3b0fb947-d438-4a39-aaee-e6cd87db9bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'transcription_test_AimeeMullins_1249s.txt' found.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "Model is running on cpu\n",
      "Loss shape: torch.Size([2])\n",
      "Reduced Loss: 2.48138427734375\n",
      "Predicted Labels: [0, 0, 0, 0, 0]\n",
      "Epoch [1/2], Loss: 0.4963\n",
      "Loss shape: torch.Size([2])\n",
      "Reduced Loss: 6.814789295196533\n",
      "Predicted Labels: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "Epoch [2/2], Loss: 1.3630\n",
      "Predicted labels: [0, 0, 0, 0, 0]\n",
      "Predicted labels is not a list of sequences. It might be a single sequence or a different format.\n",
      "Tokens: ['[CLS]', 'original', 'text', ':', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Predicted Labels: ['O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from TorchCRF import CRF\n",
    "\n",
    "# Define the path to your local file\n",
    "local_file_path = \"transcription_test_AimeeMullins_1249s.txt\"\n",
    "\n",
    "# Verify the file exists\n",
    "import os\n",
    "if not os.path.exists(local_file_path):\n",
    "    raise FileNotFoundError(f\"File not found: {local_file_path}\")\n",
    "else:\n",
    "    print(f\"File '{local_file_path}' found.\")\n",
    "\n",
    "# Load the dataset using the appropriate loader based on your file type\n",
    "tedlium = load_dataset(\"text\", data_files=local_file_path)\n",
    "\n",
    "# Verify the dataset is loaded correctly\n",
    "print(tedlium)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class TEDLIUMDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transcription = self.texts[idx]\n",
    "        return transcription\n",
    "\n",
    "# Prepare the dataset\n",
    "train_texts = tedlium['train']['text']\n",
    "\n",
    "# Creating the dataset\n",
    "train_dataset = TEDLIUMDataset(train_texts)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)  # Adjust batch size as needed\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "embed_dim = 100  # Dimension of the embedding layer\n",
    "hidden_dim = 128  # Dimension of LSTM hidden states\n",
    "output_dim = 2  # Number of classes for classification (adjust as necessary)\n",
    "\n",
    "# Define the LSTM-CRF model\n",
    "class LSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(LSTMCRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=tokenizer.pad_token_id)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply by 2 for bidirectional\n",
    "\n",
    "        # CRF layer for sequence tagging (without batch_first argument)\n",
    "        self.crf = CRF(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "\n",
    "    def decode(self, logits, mask):\n",
    "        return self.crf.decode(logits, mask=mask)\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMCRF(vocab_size, embed_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model is running on {device}\")\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Tokenize the batch of texts\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        # Placeholder for labels (replace this with actual labels)\n",
    "        labels = torch.randint(0, output_dim, (input_ids.size(0), input_ids.size(1))).to(device)  # Random labels for testing\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)  # Forward pass\n",
    "\n",
    "        # Compute CRF loss\n",
    "        try:\n",
    "            loss = -model.crf(logits, labels, mask=attention_mask.bool())  # Negative log likelihood\n",
    "            print(f\"Loss shape: {loss.shape}\")  # Print shape of loss to check\n",
    "\n",
    "            if loss.ndimension() > 0:\n",
    "                loss = loss.mean()  # Reduce to scalar if not already scalar\n",
    "\n",
    "            print(f\"Reduced Loss: {loss}\")  # Print reduced loss value\n",
    "\n",
    "            total_loss += loss.item()  # Convert to scalar and accumulate\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # After training, make predictions using the CRF layer (using viterbi_decode)\n",
    "            predicted_labels, _ = model.crf.viterbi_decode(logits, mask=attention_mask.bool())  # Decode using CRF\n",
    "            print(\"Predicted Labels:\", predicted_labels)\n",
    "            break  # Remove to evaluate on the entire dataset\n",
    "        except IndexError as e:\n",
    "            print(f\"Skipping batch with shape mismatch: logits shape {logits.shape}, labels shape {labels.shape}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)  # Average loss for the epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Define the label map for NER (BIO tagging)\n",
    "label_map = {\n",
    "    0: \"O\",          # Outside any named entity\n",
    "    1: \"B-PERSON\",   # Beginning of a person entity\n",
    "    2: \"I-PERSON\",   # Inside a person entity\n",
    "    3: \"B-ORG\",      # Beginning of an organization entity\n",
    "    4: \"I-ORG\",      # Inside an organization entity\n",
    "    # Add other entities as needed\n",
    "}\n",
    "\n",
    "# Sample prediction after training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        # Tokenize the batch of texts\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        # Decode predictions using the CRF layer\n",
    "        predicted_labels, _ = model.crf.viterbi_decode(logits, mask=attention_mask.bool())  # CRF decoding\n",
    "\n",
    "        # Debugging step: Print the predicted_labels to understand its structure\n",
    "        print(\"Predicted labels:\", predicted_labels)  # Inspect the actual content of predicted_labels\n",
    "\n",
    "        # Check if predicted_labels is a list of lists (batch of sequences) or just a single sequence\n",
    "        if isinstance(predicted_labels, list) and isinstance(predicted_labels[0], list):\n",
    "            print(\"Predicted labels shape:\", len(predicted_labels[0]))  # First sequence in the batch\n",
    "        else:\n",
    "            print(\"Predicted labels is not a list of sequences. It might be a single sequence or a different format.\")\n",
    "\n",
    "        # Convert input_ids to tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())  # Convert the first batch to tokens\n",
    "\n",
    "        # If predicted_labels is a list, we map each label to its class (BIO tags for NER)\n",
    "        if isinstance(predicted_labels, list):\n",
    "            # If it's a single sequence, map it directly\n",
    "            predicted_labels_mapped = [label_map[label] for label in predicted_labels]\n",
    "        else:\n",
    "            # If it's a single batch sequence, handle it differently\n",
    "            predicted_labels_mapped = [label_map[label.item()] for label in predicted_labels]\n",
    "\n",
    "        # Display tokens and their predicted labels (BIO tags)\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Predicted Labels:\", predicted_labels_mapped)\n",
    "\n",
    "        break  # Remove to evaluate on the entire dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a68654",
   "metadata": {
    "id": "44a68654"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
