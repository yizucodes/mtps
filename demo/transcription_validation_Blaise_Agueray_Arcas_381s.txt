Split: validation
Speaker ID: Blaise_Agueray_Arcas
Duration: 6.35 minutes
Processing method: 30-second chunks with 2-second overlap

Original text:
 what i'm going to show you first as quickly as i can is is some some foundational work some some new technology that we brought to microsoft as part of an acquisition almost exactly a year ago this is seadragon and it's an environment in which you can either locally or remotely interact with vast amounts of of visual data we're looking at many many gigabytes of of digital photos here and kind of seam seamlessly and continuously zooming in panning through the thing rearranging it in any way we want and it doesn't matter how much information we're looking at how big these collections are or how big the images are in the most of them are ordinary digital camera photos but this one for example is a scan from the library of congress and it's in the in the three hundred megapixel range it doesn't doesn't make any difference because the only thing that that ought to limit the performance of a system like this one is the number of pixels on your screen at any given moment it's also very flexible architecture this is an entire book this is an example of non non image data this is bleak house by dickens every every column is a is a chapter and to prove to you that it's that it's really text and not an image we can do something like so to really show that this is a real representation of the text it's not a picture maybe this is a kind of an artificial way to read an e book i wouldn't recommend it this is a more realistic case this is an issue of the guardian every large image is the beginning of a section and this really gives you the the the joy and the good experience of reading the real paper version of of a magazine or a newspaper which is an inherently multi scale kind of medium we've also done a little something with the corner of of this particular issue of the guardian we've made up a fake ad that's very high resolution much higher than you'd be able to get in an ordinary ad and we've embedded extra content if you want to see the features of this car you can see it here or other models or even technical specifications and and this this really this really gets at some of these ideas about really doing away with with those limits on on screen real estate we hope that this means no more no more pop ups and other kind of rubbish like that shouldn't be necessary of course mapping is one of those really obvious applications for a technology like this and and this one i really won't spend any time on except to say that we have things to contribute to this field as well but those are all the roads in the in the u s superimposed on top of a nasa geospatial image so let's pull up now something else so this is actually live on the live on the web now you can go check it out this is a project called photosynth which really marries two different technologies one of them is seadragon and the other is some very beautiful computer vision research done by noah snavely a graduate student at the university of washington co advised by steve seitz at u w and rick szeliski at microsoft research a very nice collaboration and so this is this live on the web it's powered by seadragon you can see that when we kind of do these sorts of views where we can we can we can dive through images and have this kind of multi resolution experience but but the spatial arrangement of the images here is actually meaningful the computer vision algorithms have registered these images together so that they correspond to the real space in which these these shots all taken near grassi lakes in the canadian rockies all these shots were taken so you see elements here of of stabilized slide show or panoramic panoramic imaging and these things have all been related related spatially i'm not sure if i have if i have time to show you any other environments there are some that are much more spatial but i would like to jump straight to to one of noah's original data sets and this is from an early prototype of photosynth that we first got working in the summer to show you what i think is really the the punchline behind this this technology the photosynth technology and it's not necessarily so apparent from looking at the environments that we've put up on the website we we had to worry about the lawyers and so on this is a reconstruction of notre dame cathedral that was done entirely computationally from images scraped from flickr you just type notre dame into flickr and you get some pictures of guys in t shirts and of the campus and so on and each of these orange cones represents an image that was that was discovered to belong to this model and so these are all these are all flickr images and they've all been related spatially in this way and we can just navigate in this very simple way you know i never i never thought that i'd end up working at microsoft it's very it's very gratifying to to have this kind this kind of reception here so so this is i guess you can see this is very this is lots of different types of cameras it's everything from cell phone cameras to professional slrs quite a large number of them stitched together in this environment and if i can i'll find some of the sort of weird ones so many of them are occluded by faces and and so on there's somewhere in here there is actually there there is series of photographs here we go this is actually a poster of notre dame that registered correctly here we can dive in from the poster to a physical view of this of this environment what what the point here really is is that we can do things with the social environment this is this is now taking data from everybody from the entire collective memory of of visually what the earth looks like and link all of that together all of those photos become linked together and they make something emergent that's greater than the sum of the parts you have a a model that emerges of the entire earth think of this as the long tail to stephen lawler's virtual earth work and this is something that grows in complexity as people use it and whose benefits become greater to the users as they as they use it their own photos are getting tagged with meta data that somebody else entered if if somebody bothered to to tag all of these saints and say who they all are then then my photo of notre dame cathedral suddenly gets enriched with all of that data and i can use it as an entry point to dive into that space into that meta verse using everybody else's photos and and do a kind of a cross modal and and and cross user social experience that way and of course a by product of all of that is immensely rich virtual models of of every interesting part of the earth collected not just from from overhead flights and from satellite images and so on but from the collective memory thank you so much yes what this is really doing is discovering it's creating hyperlinks if you will between between images and it's doing that based on the content inside the images and that gets really exciting when you think about the richness of the semantic information that a lot of those images have like when you do a web search for images you type in phrases and the text on the web page is is carrying a lot of information about what that picture is of now what if that picture links to all of your pictures then the amount of semantic interconnection and the amount of richness that comes out of that is really huge it's a classic network effect thanks so much

Whisper transcription:
What I'm going to show you first as quickly as I can is some foundational work, some new technology that we brought to Microsoft as part of an acquisition almost exactly a year ago. This is seedragon, and it's an environment in which you can either locally or remotely interact with vast amounts of visual data. We're looking at many, many gigabytes of digital photos here and kind of seamlessly and continuously zooming and panning through the thing, rearranging it any way we want. And it doesn't matter how much it doesn't matter how much information we're looking at, how big these collections are, or how big the images are, and the most of them are ordinary digital camera photos. But this one, for example, is a scan from the Library of Congress, and it's in the 300 megapixel range. It doesn't make any difference because the only thing that ought to limit the performance of a system like this one is the number of pixels on your screen at any given moment. It's also a very flexible architecture. This is an entire book, so this is an example of non-image data. data. This is Bleak House by Dickens. Every column is a chapter, and to prove to you that it's really text and not an image, we can do something like so, to really show that this is a real representation of the text. It's not a picture. Maybe this is kind of an artificial way to read an e-book, I wouldn't recommend it. This a more realistic case, this is an issue of the Guardian. Every large image is the beginning of a section, and this really gives you the joy and the good experience of reading the real paper version of the real paper version of a magazine or a newspaper, which is an inherently multi-scale kind of medium. We've also done a little something with the corner of this particular issue of the Guardian we've made up a fake ad that's very high resolution, much higher than you'd be able to get in an ordinary ad. And we've embedded extra content about to see the features of this car you can see it here, or other models, or even technical specifications. And this really gets at some of these ideas about really doing away about really doing away with those limits on screen real estate. We hope that this means no more pop-ups and other kind of rubbish like that should be necessary. Of course, mapping is one of those really obvious applications for technology like this. And this one I really won't spend any time on except to say that we have things to contribute to this field as well. But those are all the roads in the U.S. superimposed on top of a NASA geospatial image. So let's pull up now something else. So this is actually live on the web. Now you can go check it out. This is a project called Photosynth, which really marries two different technologies. One of them is Seedragon, and the other is some very beautiful computer vision research done by Noah Snaveli, a graduate student in the University of Washington, co-advised by Steve Sites at UDUB and Rick Solisky at Microsoft Research. A very nice collaboration. And so this is live on the web. It's powered by Seedragan. You can see that when we kind of do these sorts of views, where we can dive through images and have this kind of multi-resolution experience. But the spatial arrangement of the images here is actually meaningful. The computer vision algorithms have registered these images together so that they correspond to the real space in which these shots all taken near grassy lakes in the Canadian Rockies. All these shots were taken. So you see elements here of stabilized slideshow or panoramic imaging. And these things have all been related spatially. I'm not sure if I have time to show you any other environments. There are some that are much more to show you any other environments. There are some that are much more spatial. But I'd like to jump straight to one of Noah's original data sets. And this is from an early prototype of photosynth that we first got working in the summer. To show you what I think is really the punchline behind this technology, the photosynth technology. And it's not necessarily so apparent from looking at the environments that we've put up on the website. We had to worry about the lawyers and so on. This is a reconstruction of Notre Dame Cathedral that was done entirely computationally from images scraped from flicker. It was typed Notre Dame into flicker, And you get some pictures of guys in t-shirts, and of the campus, and so on. And each of these orange cones represents an image that was discovered to belong to this model. And so these are all flicker images. And they've all been related spatially in this way. And we can just navigate in this very simple way. I never thought that I'd end up working at Microsoft. It was very gratifying to have this kind of reception here. This is lots of different types of cameras. It's everything from cell phone cameras to professional SLRs. Quite a large number that have been registered together into this environment. If I can find some of the sort of weird ones, so many of them are occluded by faces and so on. There's somewhere in here, there's actually a series of photographs. Here we go. This is actually a poster of Notre Dame that registered correctly. We can dive in from the poster to a physical view of this information. of this environment. What the point here really is is that we can do things with the social environment. This is now taking data from everybody, from the entire collective memory of visually, of what the Earth looks like, and link all of that together. All of those photos become linked together, and they make something emergent that's greater than the sum of the parts. You have a model that emerges of the entire Earth, think of this as the long tail to Stephen Lawler's virtual Earth work, and this is something that grows in complexity as people use it. that grows in complexity as people use it and whose benefits become greater to the users as they use it. Their own photos are getting tagged with metadata that somebody else entered. If somebody bothered to tag all of these saints and say who they all are, then my photo of Notre Dame Cathedral suddenly gets enriched with all that data. And I can use it as an entry point to dive into that space and that metaverse using everybody else's photos and do a kind of cross-model and cross-user social and cross user social experience that way. And of course, a byproduct of all of that is immensely rich virtual models of every interesting part of the earth collected not just from overhead flights and from satellite images and so on, but from the collective memory. Thank you so much. Yes, what this is really doing is discovering. It's creating hyperlinks, if you will, between images and it's doing that based on the content inside the images. And that gets really exciting when you think about the richness of the semantic information that a lot of those images have. Like when you do a web search, of those images have, like when you do a web search for images, that you type in phrases and the text on the web page is carrying a lot of information about what that picture is of. Now, what if that picture links to all of your pictures, then the amount of semantic interconnection and the amount richness that comes out of that is really huge. It's a classic network effect. Thanks so much.
